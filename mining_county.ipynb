{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elephant-xyz/notebook/blob/main/mining_county.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#County Mining process"
      ],
      "metadata": {
        "id": "jUcGqCQnZW9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Upload .env"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PI_XjroKd45G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Upload seed-results.csv"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1Btx2OEDfVtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3: Prepare\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from urllib.parse import urlencode\n",
        "from typing import Optional, Dict, Any\n",
        "import traceback\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PropertyDataProcessor:\n",
        "    def __init__(self, input_csv_path: str = \"seed-results.csv\", seed_csv_path: str = \"seed.csv\"):\n",
        "        self.input_csv_path = input_csv_path\n",
        "        self.seed_csv_path = seed_csv_path\n",
        "        self.processed_parcels = []  # Track all processed parcel IDs\n",
        "        self.ipfs_gateways = [\n",
        "            \"https://ipfs.io/ipfs/\",\n",
        "            \"https://gateway.pinata.cloud/ipfs/\",\n",
        "            \"https://cloudflare-ipfs.com/ipfs/\",\n",
        "            \"https://dweb.link/ipfs/\",\n",
        "            \"https://ipfs.infura.io/ipfs/\"\n",
        "        ]\n",
        "\n",
        "    def fetch_from_ipfs(self, cid: str) -> Optional[Dict[Any, Any]]:\n",
        "        \"\"\"Fetch data from IPFS using the provided CID with multiple gateway fallback.\"\"\"\n",
        "        for gateway in self.ipfs_gateways:\n",
        "            try:\n",
        "                url = f\"{gateway}{cid}\"\n",
        "                logger.info(f\"Trying to fetch {cid} from {gateway}\")\n",
        "                response = requests.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                return response.json()\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error fetching from {gateway}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.error(f\"Failed to fetch data from IPFS CID {cid} from all gateways\")\n",
        "        return None\n",
        "\n",
        "    def trace_ipfs_chain(self, data_cid: str) -> Optional[Dict[Any, Any]]:\n",
        "        \"\"\"Trace through the IPFS chain to get the final property data.\"\"\"\n",
        "\n",
        "        # Step 1: Fetch the initial data using dataCid\n",
        "        logger.info(f\"Step 1: Fetching initial data from dataCid: {data_cid}\")\n",
        "        initial_data = self.fetch_from_ipfs(data_cid)\n",
        "        if not initial_data:\n",
        "            return None\n",
        "\n",
        "        # Step 2: Extract property_seed CID from relationships\n",
        "        try:\n",
        "            property_seed_cid = initial_data[\"relationships\"][\"property_seed\"][\"/\"]\n",
        "            logger.info(f\"Step 2: Found property_seed CID: {property_seed_cid}\")\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Could not find property_seed CID in initial data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 3: Fetch property_seed data\n",
        "        logger.info(f\"Step 3: Fetching property_seed data from: {property_seed_cid}\")\n",
        "        property_seed_data = self.fetch_from_ipfs(property_seed_cid)\n",
        "        if not property_seed_data:\n",
        "            return None\n",
        "\n",
        "        # Step 4: Extract \"to\" CID from property_seed data\n",
        "        try:\n",
        "            to_cid = property_seed_data[\"to\"][\"/\"]\n",
        "            logger.info(f\"Step 4: Found 'to' CID: {to_cid}\")\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Could not find 'to' CID in property_seed data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 5: Fetch final property data\n",
        "        logger.info(f\"Step 5: Fetching final property data from: {to_cid}\")\n",
        "        final_data = self.fetch_from_ipfs(to_cid)\n",
        "\n",
        "        return final_data\n",
        "\n",
        "    def create_seed_csv(self):\n",
        "        \"\"\"Read the input CSV, trace IPFS chain, and create seed.csv.\"\"\"\n",
        "\n",
        "        # Read the input CSV\n",
        "        try:\n",
        "            df = pd.read_csv(self.input_csv_path)\n",
        "            logger.info(f\"Loaded {len(df)} records from {self.input_csv_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading CSV file: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Prepare output data\n",
        "        output_rows = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            data_cid = row['dataCid']\n",
        "            logger.info(f\"Processing row {index + 1}: {data_cid}\")\n",
        "\n",
        "            # Trace the IPFS chain\n",
        "            final_data = self.trace_ipfs_chain(data_cid)\n",
        "\n",
        "            if final_data:\n",
        "                try:\n",
        "                    # Extract data for CSV\n",
        "                    parcel_id = final_data.get('request_identifier', '')\n",
        "                    address = final_data.get('full_address', '')\n",
        "                    county = final_data.get('county_jurisdiction', '')\n",
        "\n",
        "                    # Track this parcel ID\n",
        "                    if parcel_id:\n",
        "                        self.processed_parcels.append(parcel_id)\n",
        "\n",
        "                    # Extract HTTP request details\n",
        "                    http_request = final_data.get('source_http_request', {})\n",
        "                    method = http_request.get('method', '')\n",
        "                    url = http_request.get('url', '')\n",
        "                    multi_value_query_string = http_request.get('multiValueQueryString', {})\n",
        "\n",
        "                    # Convert multiValueQueryString to JSON string for CSV\n",
        "                    multi_value_query_string_str = json.dumps(multi_value_query_string) if multi_value_query_string else ''\n",
        "\n",
        "                    # Create output row\n",
        "                    output_row = {\n",
        "                        'parcel_id': parcel_id,\n",
        "                        'Address': address,\n",
        "                        'method': method,\n",
        "                        'headers': '',  # Empty as per example\n",
        "                        'url': url,\n",
        "                        'multiValueQueryString': multi_value_query_string_str,\n",
        "                        'body': '',  # Empty as per example\n",
        "                        'json': '',  # Empty as per example\n",
        "                        'source_identifier': parcel_id,  # Same as parcel_id based on example\n",
        "                        'County': county\n",
        "                    }\n",
        "\n",
        "                    output_rows.append(output_row)\n",
        "                    logger.info(f\"Successfully processed parcel ID: {parcel_id}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing final data for row {index + 1}: {e}\")\n",
        "            else:\n",
        "                logger.error(f\"Failed to trace IPFS chain for row {index + 1}\")\n",
        "\n",
        "        # Create output DataFrame and save to CSV\n",
        "        if output_rows:\n",
        "            output_df = pd.DataFrame(output_rows)\n",
        "            output_df.to_csv(self.seed_csv_path, index=False)\n",
        "            return True\n",
        "        else:\n",
        "            logger.error(\"No data was successfully processed\")\n",
        "            print(\"No data was successfully processed\")\n",
        "            return False\n",
        "\n",
        "    def create_output_directory(self):\n",
        "        \"\"\"Create the input directory if it doesn't exist\"\"\"\n",
        "        if not os.path.exists('input'):\n",
        "            os.makedirs('input')\n",
        "            logger.info(\"Created 'input' directory\")\n",
        "\n",
        "    def parse_multi_value_query_string(self, query_string_json):\n",
        "        \"\"\"Parse the multiValueQueryString JSON and convert to URL parameters\"\"\"\n",
        "        try:\n",
        "            if not query_string_json or query_string_json.strip() == '':\n",
        "                return {}\n",
        "\n",
        "            query_data = json.loads(query_string_json)\n",
        "            # Convert multi-value query string to regular query parameters\n",
        "            params = {}\n",
        "            for key, values in query_data.items():\n",
        "                if isinstance(values, list) and len(values) > 0:\n",
        "                    params[key] = values[0]  # Take the first value\n",
        "                else:\n",
        "                    params[key] = values\n",
        "            return params\n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.error(f\"Error parsing query string JSON: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def make_request(self, row):\n",
        "        \"\"\"Make HTTP request based on CSV row data\"\"\"\n",
        "        try:\n",
        "            parcel_id = row['parcel_id']\n",
        "            address = row['Address']\n",
        "            method = row['method'].upper()\n",
        "            url = row['url']\n",
        "            query_params = self.parse_multi_value_query_string(row['multiValueQueryString'])\n",
        "\n",
        "            logger.info(f\"Processing parcel {parcel_id} at {address}\")\n",
        "\n",
        "            # Use headers from CSV if provided, otherwise use minimal headers\n",
        "            request_headers = {}\n",
        "            if row.get('headers') and row['headers'].strip():\n",
        "                try:\n",
        "                    request_headers = json.loads(row['headers'])\n",
        "                except json.JSONDecodeError:\n",
        "                    logger.warning(f\"Invalid headers JSON for parcel {parcel_id}, using minimal headers\")\n",
        "\n",
        "            # If no headers provided or parsing failed, use minimal headers\n",
        "            if not request_headers:\n",
        "                request_headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                }\n",
        "\n",
        "            # Make the request\n",
        "            if method == 'GET':\n",
        "                response = requests.get(url, params=query_params, headers=request_headers, timeout=30)\n",
        "            elif method == 'POST':\n",
        "                # Handle POST request with body if provided\n",
        "                post_data = {}\n",
        "                if row.get('body') and row['body'].strip():\n",
        "                    try:\n",
        "                        post_data = json.loads(row['body'])\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"Invalid body JSON for parcel {parcel_id}, using empty body\")\n",
        "\n",
        "                # Handle JSON data if provided\n",
        "                if row.get('json') and row['json'].strip():\n",
        "                    try:\n",
        "                        json_data = json.loads(row['json'])\n",
        "                        response = requests.post(url, params=query_params, headers=request_headers, json=json_data, timeout=30)\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"Invalid JSON data for parcel {parcel_id}, using form data\")\n",
        "                        response = requests.post(url, params=query_params, headers=request_headers, data=post_data, timeout=30)\n",
        "                else:\n",
        "                    response = requests.post(url, params=query_params, headers=request_headers, data=post_data, timeout=30)\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported method {method} for parcel {parcel_id}\")\n",
        "                return False\n",
        "\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Save the HTML content\n",
        "            filename = f\"input/{parcel_id}.html\"\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(response.text)\n",
        "\n",
        "            logger.info(f\"Successfully saved {filename}\")\n",
        "            return True\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Request failed for parcel {parcel_id}: {e}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error processing parcel {parcel_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_property_data(self):\n",
        "        \"\"\"Read seed CSV and download property data for each parcel\"\"\"\n",
        "        successful_downloads = 0\n",
        "        failed_downloads = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.seed_csv_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "\n",
        "                # Print available columns for debugging\n",
        "                logger.info(f\"Available columns: {reader.fieldnames}\")\n",
        "\n",
        "                # Verify required columns exist\n",
        "                required_columns = ['parcel_id', 'Address', 'method', 'url', 'multiValueQueryString']\n",
        "                missing_columns = [col for col in required_columns if col not in reader.fieldnames]\n",
        "                if missing_columns:\n",
        "                    logger.error(f\"Missing required columns: {missing_columns}\")\n",
        "                    return False\n",
        "\n",
        "                logger.info(f\"Starting to process seed CSV file: {self.seed_csv_path}\")\n",
        "\n",
        "                # Convert reader to list to see total count\n",
        "                rows = list(reader)\n",
        "                total_rows = len(rows)\n",
        "                logger.info(f\"Found {total_rows} rows to process\")\n",
        "\n",
        "                for row_num, row in enumerate(rows, start=1):\n",
        "                    logger.info(f\"Processing row {row_num}/{total_rows} - Parcel: {row.get('parcel_id', 'Unknown')}\")\n",
        "\n",
        "                    try:\n",
        "                        if self.make_request(row):\n",
        "                            successful_downloads += 1\n",
        "                            logger.info(f\"✓ Successfully processed parcel {row.get('parcel_id')}\")\n",
        "                        else:\n",
        "                            failed_downloads += 1\n",
        "                            logger.error(f\"✗ Failed to process parcel {row.get('parcel_id')}\")\n",
        "                    except Exception as e:\n",
        "                        failed_downloads += 1\n",
        "                        logger.error(f\"✗ Exception processing parcel {row.get('parcel_id')}: {e}\")\n",
        "\n",
        "                    # Add a small delay to be respectful to the server\n",
        "                    time.sleep(1)\n",
        "\n",
        "            logger.info(f\"Download complete. Successful: {successful_downloads}, Failed: {failed_downloads}\")\n",
        "            return True\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Seed CSV file '{self.seed_csv_path}' not found\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing seed CSV file: {e}\")\n",
        "            logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def run_complete_process(self):\n",
        "        \"\"\"Run the complete process: IPFS data fetching + property download\"\"\"\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"STARTING COMPLETE PROPERTY DATA PROCESSING\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        # Step 1: Create seed CSV from IPFS data\n",
        "        logger.info(\"STEP 1: Processing IPFS data to create seed CSV...\")\n",
        "        if not self.create_seed_csv():\n",
        "            logger.error(\"Failed to create seed CSV. Aborting.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"STEP 1 COMPLETED: Seed CSV created successfully\")\n",
        "        logger.info(\"-\" * 40)\n",
        "\n",
        "        # Step 2: Create output directory for HTML files\n",
        "        logger.info(\"STEP 2: Creating output directory...\")\n",
        "        self.create_output_directory()\n",
        "        logger.info(\"STEP 2 COMPLETED: Output directory ready\")\n",
        "        logger.info(\"-\" * 40)\n",
        "\n",
        "        # Step 3: Download property data\n",
        "        logger.info(\"STEP 3: Downloading property data from county websites...\")\n",
        "        if not self.download_property_data():\n",
        "            logger.error(\"Failed to download property data.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"STEP 3 COMPLETED: Property data download finished\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"COMPLETE PROCESS FINISHED SUCCESSFULLY\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the complete property data processor\"\"\"\n",
        "\n",
        "    # Initialize the processor with default file paths\n",
        "    # You can modify these paths as needed\n",
        "    processor = PropertyDataProcessor(\n",
        "        input_csv_path=\"seed-results.csv\",  # Input CSV with dataCid column\n",
        "        seed_csv_path=\"seed.csv\"              # Output seed CSV and input for downloads\n",
        "    )\n",
        "\n",
        "    # Run the complete process\n",
        "    success = processor.run_complete_process()\n",
        "\n",
        "    if success:\n",
        "        # Show all processed parcel IDs\n",
        "        if processor.processed_parcels:\n",
        "            parcels_str = \", \".join(processor.processed_parcels)\n",
        "            print(f\"✅ Prepare done for parcel IDs: {parcels_str}\")\n",
        "        else:\n",
        "            print(\"✅ Prepare done (no parcel IDs found)\")\n",
        "    else:\n",
        "        print(\"❌ Prepare Failed\")\n",
        "        print(\"Check the logs above for detailed error information\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "p9NoMY6ufmSE",
        "outputId": "f06f5e22-9dae-4a42-e39b-cb03fda98b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Prepare done for parcel IDs: 52434205310037080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Transform\n",
        "#!/usr/bin/env python3\n",
        "import subprocess\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies\"\"\"\n",
        "    try:\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\", \"-q\"\n",
        "        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        return True\n",
        "    except:\n",
        "        print(\"❌ Failed to install dependencies\")\n",
        "        return False\n",
        "\n",
        "def load_environment():\n",
        "    \"\"\"Load environment variables from .env file\"\"\"\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"❌ Failed to import dotenv\")\n",
        "        return False\n",
        "\n",
        "def check_env_file():\n",
        "    \"\"\"Check if .env file exists\"\"\"\n",
        "    if not os.path.exists(\".env\"):\n",
        "        print(\"❌ Transform failed: .env file not found\")\n",
        "        print(\"Please upload the .env file to continue\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def copy_seed_results():\n",
        "    \"\"\"Copy seed-results.csv to upload-results.csv\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(\"seed-results.csv\"):\n",
        "            print(\"❌ Transform failed: seed-results.csv not found\")\n",
        "            return False\n",
        "\n",
        "        shutil.copy2(\"seed-results.csv\", \"upload-results.csv\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Transform failed: Could not copy seed-results.csv\")\n",
        "        return False\n",
        "\n",
        "def run_transform():\n",
        "    \"\"\"Run the transform command and suppress logs\"\"\"\n",
        "\n",
        "    command = [\n",
        "        \"uvx\",\n",
        "        \"--from\",\n",
        "        \"git+https://github.com/elephant-xyz/AI-Agent\",\n",
        "        \"test-evaluator-agent\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(\"🔄 Transforming running...\")\n",
        "\n",
        "        # Run the command and suppress all output\n",
        "        result = subprocess.run(\n",
        "            command,\n",
        "            stdout=subprocess.DEVNULL,  # Suppress stdout\n",
        "            stderr=subprocess.DEVNULL,  # Suppress stderr\n",
        "            check=True  # Raise exception if command fails\n",
        "        )\n",
        "\n",
        "        return True\n",
        "\n",
        "    except:\n",
        "        print(\"❌ Transform failed\")\n",
        "        return False\n",
        "\n",
        "def get_seed_cid_and_html_link(path=\"county-results.csv\"):\n",
        "    \"\"\"Get County CID and HTML link from CSV file\"\"\"\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row[\"dataGroupCid\"], first_row[\"htmlLink\"]\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    \"\"\"\n",
        "    Returns True if submit_errors.csv has at least one row (after header).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            return next(reader, None) is not None\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "\n",
        "def run_validate_and_upload():\n",
        "    \"\"\"Run validation and upload process\"\"\"\n",
        "    try:\n",
        "\n",
        "        subprocess.run(\n",
        "            [\"npx\", \"-y\", \"@elephant-xyz/cli\", \"validate-and-upload\", \"submit\", \"--output-csv\", \"county-results.csv\"],\n",
        "            stdout=subprocess.DEVNULL,    # hide stdout\n",
        "            stderr=subprocess.PIPE,       # capture stderr\n",
        "            check=True,\n",
        "            text=True                     # stderr as string\n",
        "        )\n",
        "\n",
        "        # If there are recorded errors - stop execution\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Transform failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return False\n",
        "\n",
        "        # Otherwise - read results\n",
        "        seed_group_cid, html_link = get_seed_cid_and_html_link()\n",
        "        print(\"✅ Transform done\\n\")\n",
        "        print(f\"county group CID: {seed_group_cid}\\n\")\n",
        "        print(f\"HTML link: {html_link}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # handle command execution errors\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        if e.stderr:\n",
        "            print(e.stderr.strip(), file=sys.stderr)\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Validation and upload failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    # Step 1: Install dependencies\n",
        "    if not install_dependencies():\n",
        "        return False\n",
        "\n",
        "    # Step 2: Load environment\n",
        "    if not load_environment():\n",
        "        return False\n",
        "\n",
        "    # Step 3: Check for .env file\n",
        "    if not check_env_file():\n",
        "        return False\n",
        "\n",
        "    # Step 4: Copy seed-results.csv to upload-results.csv\n",
        "    if not copy_seed_results():\n",
        "        return False\n",
        "\n",
        "    # Step 5: Run the transform command\n",
        "    if not run_transform():\n",
        "        return False\n",
        "\n",
        "    # Step 6: Run validation and upload\n",
        "    success = run_validate_and_upload()\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    # Don't use sys.exit() in Jupyter/IPython environments\n",
        "\n",
        "    if not success:\n",
        "        print(\"Process completed with errors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "DyG0lJvbmcej",
        "outputId": "bfecf0a5-61e3-454e-df35-d5b45b7b4bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Transform done\n",
            "\n",
            "county group CID: bafkreie5pbx4k3wt3fnd4qewthsde2jxewm3krcgn72ecbyvnzqhaeylce\n",
            "\n",
            "HTML link: http://dweb.link/ipfs/bafybeibrz2cupddfmxuzmk4b5ghun32hno4oyw2ehtooyoipottm7qeivu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: Validate\n",
        "! pip3 install python-dotenv -q\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "\n",
        "def get_seed_cid_and_html_link(path=\"county-results.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row[\"dataGroupCid\"], first_row[\"htmlLink\"]\n",
        "\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    \"\"\"\n",
        "    Повертає True, якщо у файлі submit_errors.csv є хоча б один рядок (після заголовку).\n",
        "    \"\"\"\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return next(reader, None) is not None\n",
        "\n",
        "\n",
        "def run_validate_and_upload():\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"npx\", \"-y\", \"@elephant-xyz/cli\", \"validate-and-upload\", \"submit\", \"--output-csv\", \"county-results.csv\"],\n",
        "            stdout=subprocess.DEVNULL,    # ховаємо stdout\n",
        "            stderr=subprocess.PIPE,       # ловимо stderr у буфер\n",
        "            check=True,\n",
        "            text=True                     # stderr як рядок\n",
        "        )\n",
        "        # Якщо є записані помилки — завершуємо роботу\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Validate failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        # Інакше — читаємо результати\n",
        "        seed_group_cid, html_link = get_seed_cid_and_html_link()\n",
        "        print(\"✅ Validate done\\n\")\n",
        "        print(f\"County group CID: {seed_group_cid}\\n\")\n",
        "        print(f\"HTML link: {html_link}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # обробка помилок виконання команди\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        print(e.stderr.strip(), file=sys.stderr)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_validate_and_upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "82AWqrDApExu",
        "outputId": "0dff6038-c646-420d-a9cf-d013662ab6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validate done\n",
            "\n",
            "County group CID: bafkreiequh74xoafkabgtqnsyvynus5x6wmmh3k67v6cqrusplkhxuyvke\n",
            "\n",
            "HTML link: http://dweb.link/ipfs/bafybeia4j33blycalqtf3pxtzq5k5iams7ax7lbaxblyk5p2cwm5xfedhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6: Upload\n",
        "! pip3 install python-dotenv requests -q\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def get_seed_info(path=\"county-results.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row\n",
        "\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    \"\"\"\n",
        "    Повертає True, якщо у файлі submit_errors.csv є хоча б один рядок (після заголовку).\n",
        "    \"\"\"\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return next(reader, None) is not None\n",
        "\n",
        "\n",
        "def count_upload_records(path=\"county-results.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return sum(1 for _ in reader)\n",
        "\n",
        "\n",
        "def fetch_with_fallback(cid, gateways=None):\n",
        "    \"\"\"\n",
        "    Try to fetch IPFS content from multiple gateways with fallback\n",
        "    \"\"\"\n",
        "    if gateways is None:\n",
        "        gateways = [\n",
        "            \"https://ipfs.io/ipfs/\",\n",
        "            \"https://gateway.pinata.cloud/ipfs/\",\n",
        "            \"https://dweb.link/ipfs/\",\n",
        "            \"https://cloudflare-ipfs.com/ipfs/\"\n",
        "        ]\n",
        "\n",
        "    for gateway in gateways:\n",
        "        try:\n",
        "            url = f\"{gateway}{cid}\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200 and response.text.strip():\n",
        "                return response\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # If all gateways fail, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def collect_data_ipfs_links(data_cid):\n",
        "    \"\"\"\n",
        "    Collect IPFS links for County data structure\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = fetch_with_fallback(data_cid)\n",
        "        if response is None:\n",
        "            print(f\"Error: Could not fetch seed data from any gateway for CID: {data_cid}\", file=sys.stderr)\n",
        "            return {}\n",
        "        seed_data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching seed data: {e}\", file=sys.stderr)\n",
        "        return {}\n",
        "\n",
        "    entity_links = {}  # For actual entities (person, property, etc.)\n",
        "    relationship_links = {}  # For relationship objects\n",
        "    url_to_name = {}  # Track which URLs we've already seen and their preferred names\n",
        "\n",
        "    # Get relationships from the County data\n",
        "    relationships = seed_data.get(\"relationships\", {})\n",
        "\n",
        "    # Process required single-value relationships\n",
        "    for rel_name in [\"property_has_address\", \"property_has_lot\", \"property_has_structure\", \"property_has_utility\", \"property_has_flood_storm_information\"]:\n",
        "        if rel_name in relationships and relationships[rel_name]:\n",
        "            rel_cid = relationships[rel_name].get(\"/\") if isinstance(relationships[rel_name], dict) else relationships[rel_name]\n",
        "            if rel_cid:\n",
        "                # Try to get the referenced data and extract 'to' and 'from' fields FIRST\n",
        "                try:\n",
        "                    response = fetch_with_fallback(rel_cid)\n",
        "                    if response is None:\n",
        "                        print(f\"Warning: Could not fetch {rel_name} from any gateway: {rel_cid}\", file=sys.stderr)\n",
        "                        continue\n",
        "\n",
        "                    rel_data = response.json()\n",
        "\n",
        "                    if \"to\" in rel_data and \"/\" in rel_data[\"to\"]:\n",
        "                        to_cid = rel_data[\"to\"][\"/\"]\n",
        "                        # Extract x and y from x_has_y pattern\n",
        "                        parts = rel_name.split(\"_has_\")\n",
        "                        x = parts[0] if len(parts) > 0 else \"unknown\"\n",
        "                        y = parts[1] if len(parts) > 1 else \"unknown\"\n",
        "\n",
        "                        # Handle 'from' URL\n",
        "                        if \"from\" in rel_data and \"/\" in rel_data[\"from\"]:\n",
        "                            from_cid = rel_data['from']['/']\n",
        "                            from_url = f\"https://ipfs.io/ipfs/{from_cid}\"\n",
        "                            if from_url not in url_to_name:\n",
        "                                url_to_name[from_url] = x\n",
        "                                entity_links[x] = from_url\n",
        "\n",
        "                        # Handle 'to' URL\n",
        "                        to_url = f\"https://ipfs.io/ipfs/{to_cid}\"\n",
        "                        if to_url not in url_to_name:\n",
        "                            url_to_name[to_url] = y\n",
        "                            entity_links[y] = to_url\n",
        "\n",
        "                    # Add the relationship link itself AFTER processing entities\n",
        "                    relationship_links[rel_name] = f\"https://ipfs.io/ipfs/{rel_cid}\"\n",
        "\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: JSON decode error for {rel_name}: {e}\", file=sys.stderr)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not fetch relationship data for {rel_name}: {e}\", file=sys.stderr)\n",
        "\n",
        "    # Process array relationships\n",
        "    array_rels = [\n",
        "        \"company_has_property\",\n",
        "        \"person_has_property\",\n",
        "        \"property_has_file\",\n",
        "        \"property_has_layout\",\n",
        "        \"property_has_tax\",\n",
        "        \"property_has_sales_history\",\n",
        "        \"sales_history_has_company\",\n",
        "        \"sales_history_has_person\"\n",
        "    ]\n",
        "\n",
        "    for rel_name in array_rels:\n",
        "        if rel_name in relationships and relationships[rel_name]:\n",
        "            rel_array = relationships[rel_name]\n",
        "            if isinstance(rel_array, list):\n",
        "                for i, rel_item in enumerate(rel_array):\n",
        "                    rel_cid = rel_item.get(\"/\") if isinstance(rel_item, dict) else rel_item\n",
        "                    if rel_cid:\n",
        "                        # Try to get the referenced data and extract 'to' and 'from' fields FIRST\n",
        "                        try:\n",
        "                            response = fetch_with_fallback(rel_cid)\n",
        "                            if response is None:\n",
        "                                print(f\"Warning: Could not fetch {rel_name}[{i}] from any gateway: {rel_cid}\", file=sys.stderr)\n",
        "                                continue\n",
        "\n",
        "                            rel_data = response.json()\n",
        "\n",
        "                            if \"to\" in rel_data and \"/\" in rel_data[\"to\"]:\n",
        "                                to_cid = rel_data[\"to\"][\"/\"]\n",
        "                                # Extract x and y from x_has_y pattern\n",
        "                                parts = rel_name.split(\"_has_\")\n",
        "                                x = parts[0] if len(parts) > 0 else \"unknown\"\n",
        "                                y = parts[1] if len(parts) > 1 else \"unknown\"\n",
        "\n",
        "                                # Handle 'from' URL\n",
        "                                if \"from\" in rel_data and \"/\" in rel_data[\"from\"]:\n",
        "                                    from_cid = rel_data['from']['/']\n",
        "                                    from_url = f\"https://ipfs.io/ipfs/{from_cid}\"\n",
        "                                    if from_url not in url_to_name:\n",
        "                                        # Use index for arrays to make keys unique only if needed\n",
        "                                        x_key = f\"{x}_{i+1}\" if x in entity_links else x\n",
        "                                        url_to_name[from_url] = x_key\n",
        "                                        entity_links[x_key] = from_url\n",
        "\n",
        "                                # Handle 'to' URL\n",
        "                                to_url = f\"https://ipfs.io/ipfs/{to_cid}\"\n",
        "                                if to_url not in url_to_name:\n",
        "                                    # Use index for arrays to make keys unique only if needed\n",
        "                                    y_key = f\"{y}_{i+1}\" if y in entity_links else y\n",
        "                                    url_to_name[to_url] = y_key\n",
        "                                    entity_links[y_key] = to_url\n",
        "\n",
        "                            # Add the relationship link itself with index AFTER processing entities\n",
        "                            rel_key = f\"{rel_name}_{i}\"\n",
        "\n",
        "                            # Special naming for person_has_property and company_has_property relationships\n",
        "                            if rel_name == \"person_has_property\":\n",
        "                                rel_key = f\"person_{i+1}_has_property\"\n",
        "                            elif rel_name == \"company_has_property\":\n",
        "                                rel_key = f\"company_{i+1}_has_property\"\n",
        "\n",
        "                            relationship_links[rel_key] = f\"https://ipfs.io/ipfs/{rel_cid}\"\n",
        "\n",
        "                        except ValueError as e:\n",
        "                            print(f\"Warning: JSON decode error for {rel_name}[{i}]: {e}\", file=sys.stderr)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Could not fetch relationship data for {rel_name}[{i}]: {e}\", file=sys.stderr)\n",
        "\n",
        "    # Combine entity links first, then relationship links\n",
        "    all_links = {}\n",
        "    all_links.update(entity_links)\n",
        "    all_links.update(relationship_links)\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "def run_validate_and_upload():\n",
        "    try:\n",
        "        # subprocess.run(\n",
        "        #     [\"npx\", \"-y\", \"@elephant-xyz/cli\", \"validate-and-upload\", \"submit\", \"--output-csv\", \"county-results.csv\"],\n",
        "        #     stdout=subprocess.DEVNULL,    # ховаємо stdout\n",
        "        #     stderr=subprocess.PIPE,       # ловимо stderr у буфер\n",
        "        #     check=True,\n",
        "        #     text=True,\n",
        "        # )\n",
        "\n",
        "        # if has_submit_errors():\n",
        "        #     print(\"❌ Validate failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "        #     return\n",
        "\n",
        "        # seed_info = get_seed_info()\n",
        "        seed_group_cid, data_cid, html_link = \"bafkreiequh74xoafkabgtqnsyvynus5x6wmmh3k67v6cqrusplkhxuyvke\", \"bafkreibbxj5gbcqho3iranhsrwt7j2ajgwtjb7kz62mb55moju6i6tlad4\", \"http://dweb.link/ipfs/bafybeidvm5nfq3f6akwiljfe3m5a3bhhwotpqc5i46hih56fjldyqznp3u\"\n",
        "\n",
        "        all_links = collect_data_ipfs_links(data_cid)\n",
        "\n",
        "        files_uploaded = len(all_links)\n",
        "\n",
        "        print(\"✅ Upload done\\n\")\n",
        "        print(f\"{files_uploaded} files uploaded\\n\")\n",
        "\n",
        "        print(f\"County group CID: {seed_group_cid}\\n\")\n",
        "        print(f\"HTML link: {html_link}\\n\")\n",
        "\n",
        "        # Print all collected IPFS links (now deduplicated)\n",
        "        print(\"=== IPFS Links ===\")\n",
        "        for link_name, link_url in all_links.items():\n",
        "            print(f\"{link_name}: {link_url}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        print(e.stderr.strip(), file=sys.stderr)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_validate_and_upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "IgDkOErzrfj3",
        "outputId": "2402d61b-77cb-4759-bb9b-996ebabe25d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Upload done\n",
            "\n",
            "86 files uploaded\n",
            "\n",
            "County group CID: bafkreiequh74xoafkabgtqnsyvynus5x6wmmh3k67v6cqrusplkhxuyvke\n",
            "\n",
            "HTML link: http://dweb.link/ipfs/bafybeidvm5nfq3f6akwiljfe3m5a3bhhwotpqc5i46hih56fjldyqznp3u\n",
            "\n",
            "=== IPFS Links ===\n",
            "property: https://ipfs.io/ipfs/bafkreihupabbeye3v2hpvxaplwwjdxwsavzolneab2igdw472maoczq6qa\n",
            "address: https://ipfs.io/ipfs/bafkreihytmfhn26gyuyhdc5d3ovpbi3c2kygmhnzypmojoug4e7fxgjps4\n",
            "lot: https://ipfs.io/ipfs/bafkreihgofffe3h6quj6e3l3yvyct5kdbzmybfx4wu2hfuykzhaipu3yvy\n",
            "structure: https://ipfs.io/ipfs/bafkreidbfsf45uy2rmpmlbruoi4ybez2mjmrewpy4keasoj5gn2m7xjuqy\n",
            "utility: https://ipfs.io/ipfs/bafkreibf7ollclzkkq7erazrhfbq7gvw35cju3yp2ihdzvyo5qgejoy5ji\n",
            "company: https://ipfs.io/ipfs/bafkreihgjt72wqenjirz7ie35lvvpkll42zvg3sl2tqdybqleswca5pzki\n",
            "person: https://ipfs.io/ipfs/bafkreihhfk76ycskrv5pqtwjo2lbhsh27kxmqagsk2dqxik6wgqgcaeicy\n",
            "person_2: https://ipfs.io/ipfs/bafkreigmndfysu6jtp2e6w4bpr4fjowhoba76lof7irro73ovjd2e3kncu\n",
            "person_3: https://ipfs.io/ipfs/bafkreif3mlbboolj3kuv5ldy2vciytv5waounnecz3k3cyuh5zpxd5dwmq\n",
            "person_4: https://ipfs.io/ipfs/bafkreicm62rf5p3oqugqoarvam5s445pu766aq26opei4vpuiua3qvm7ne\n",
            "person_5: https://ipfs.io/ipfs/bafkreiewmunjesgh4rb7brbcmo245fyfszvjcihvueeg5weupm6y67sfz4\n",
            "person_6: https://ipfs.io/ipfs/bafkreiauohav3zfrhu3ukh6lqu4dxuyusl2k2hmutf3m7ttye6p3hgqhpe\n",
            "person_7: https://ipfs.io/ipfs/bafkreifzfxiefcl2hf5k3u6akylvroftdot5pgzeffuh3c6lxar3xrfnym\n",
            "person_8: https://ipfs.io/ipfs/bafkreieuizgkrpkbenoa6ckszkxyusniaeooejp6acng7hqkewgqhqqrv4\n",
            "layout: https://ipfs.io/ipfs/bafkreifhufpw444zteby2xpvhs22csle7swiurhj2knzitegx4mjg5irmq\n",
            "layout_3: https://ipfs.io/ipfs/bafkreigiozz4jbxzgq5u7mkvtixliqnsinm5p6n4gap6l23cztp2j5pnei\n",
            "tax: https://ipfs.io/ipfs/bafkreihotat3kvvpvzzzxyoa7yo3dbhyeigsphwzyvasv3mgwwnrjl2ig4\n",
            "tax_2: https://ipfs.io/ipfs/bafkreiffngm7jx43mosgff5ujvg6kmc7ooqykg6syv6ytdssdfli7tz4hm\n",
            "tax_3: https://ipfs.io/ipfs/bafkreifh3ukujq43qfaydpaehtrgyxctwfs26tajihef53msxiq4k5gx2y\n",
            "tax_4: https://ipfs.io/ipfs/bafkreihr4yfgvo7icqgjfjhequhwdoirgom6oxdroril4gabnjyfrubvxi\n",
            "tax_5: https://ipfs.io/ipfs/bafkreicrvyysnyfywnla5nap2ykttxmctbyclyn6qhmixgxyteq5gg7nce\n",
            "tax_6: https://ipfs.io/ipfs/bafkreigepxmbnki3ugd5jn53zzfwpg4mnwh3mcjikav5jcxgdx4gbyzsma\n",
            "tax_7: https://ipfs.io/ipfs/bafkreifao7lixjiuq5aiyturnutysxk5uvesoflelsifl5crvdbkevi5ea\n",
            "tax_8: https://ipfs.io/ipfs/bafkreicvfof5ofz4vodceejqpo5vpayoytwwz6td3sygi3lobsgnvifoeq\n",
            "tax_9: https://ipfs.io/ipfs/bafkreihyh3a2bjeijpiikvx6bg32axcs4na26f5gw7tyojhyl3dgudnf54\n",
            "tax_10: https://ipfs.io/ipfs/bafkreieiiphh5dowcrrnj7nzekkc5mllxcewgtkkhhfqf5kftbof26tczq\n",
            "sales_history: https://ipfs.io/ipfs/bafkreigjy7nqkt2vjp2hehkrnaivsm5w4perq633wxsgyj63svayx2bmpe\n",
            "sales_history_2: https://ipfs.io/ipfs/bafkreihfsdykiswbg3msynuxetshf6axr5dbbsvcyhiq4ihwh5rzls4uce\n",
            "sales_history_3: https://ipfs.io/ipfs/bafkreib4vxrjhwu3rbs35xudjvpyqzzlejsj34tifekhz5d45zy2jq5q44\n",
            "sales_history_4: https://ipfs.io/ipfs/bafkreibrrglomzd35bjxy47b4cikckanpk5xtmf4kgax5sfh2pjvcr53gm\n",
            "sales_history_5: https://ipfs.io/ipfs/bafkreiam5dgyy74scjrpk2hgguavrpnmir4osc7y2xjtavautn5m6gfpey\n",
            "sales_history_6: https://ipfs.io/ipfs/bafkreic7cyvaonrtn6uht76h22z3fd34benaqstswztj2qokqfh6scbja4\n",
            "sales_history_7: https://ipfs.io/ipfs/bafkreigqyevxyuixjd37nytwaa35yq2wjdwa2xybe7jyxh7hnaiulgitba\n",
            "sales_history_8: https://ipfs.io/ipfs/bafkreiae5ws37qzkwsz3n6jp5ba3g574be366ylfmyddr2ph733t7r4v4q\n",
            "sales_history_9: https://ipfs.io/ipfs/bafkreifsltsoqfx4dt4okmiqmlmiad3epde4etdi6txnymfdqtqfy7he7m\n",
            "sales_history_10: https://ipfs.io/ipfs/bafkreifm2dnnesnmnrl6ldsbeqfwghdc74brhwvm7fjyrex4uxiuxo25nm\n",
            "sales_history_11: https://ipfs.io/ipfs/bafkreibdnob5nhwj57ttuuzikgxfpezkzvv3bfbi33msbc5wrvibvcf32m\n",
            "sales_history_12: https://ipfs.io/ipfs/bafkreidpuzw5cn5xtezg3ivqrnimw3a2l2s2vcmeyphv76yobey3x2hxi4\n",
            "property_has_address: https://ipfs.io/ipfs/bafkreiat523qrnlbrqfq5flwkoeh73kgqunnptia65icdwyfipb6leugfa\n",
            "property_has_lot: https://ipfs.io/ipfs/bafkreictzmdjchgicauj7lel3c24z7bsjuxmrzurqqedlad7hdyv4zip3y\n",
            "property_has_structure: https://ipfs.io/ipfs/bafkreiciosstnpgbwteklw6ksfwnwj6i4qe22hw5f4hvpy4sqeq3glq3tu\n",
            "property_has_utility: https://ipfs.io/ipfs/bafkreifl6gf6sej7ac5fyhuwz5wghgtyma2q46jozfpl4vbogvq4uqxlsi\n",
            "company_1_has_property: https://ipfs.io/ipfs/bafkreieoaewu56v5wxudvd7tsb46ehsnadpom6fxtzbx2hhmdvkdtwfoie\n",
            "person_1_has_property: https://ipfs.io/ipfs/bafkreiad67odmxh4n5qajvucsodut4wl626oiivishzyirbcyjkt6grsiu\n",
            "person_2_has_property: https://ipfs.io/ipfs/bafkreibxpojdeuds5wiyzgctum35sqli6p66ewxk4tyzbmts4rtmg42bmi\n",
            "person_3_has_property: https://ipfs.io/ipfs/bafkreif6syvkd3s4zpk75qwomyra5k44bwjfghhwrxqa2jilrckczya6je\n",
            "person_4_has_property: https://ipfs.io/ipfs/bafkreiga6gb27ambdq4yvqbrn7a2zfdkdwz46rruemdvnhqghr3feh7rum\n",
            "person_5_has_property: https://ipfs.io/ipfs/bafkreigxx7nn3nopkhjve7ss5ulfb33rchs3ln7nlsys4eel7p7v5yhrt4\n",
            "person_6_has_property: https://ipfs.io/ipfs/bafkreigyrxmegkr5fvup7pbqijopolyumt3shyx67h7o3zt3dfn6qrjeme\n",
            "person_7_has_property: https://ipfs.io/ipfs/bafkreih2m4hic6anlrblwlqgxsbmnim5lrqa3xttfrrnozkjvxbaqjcinm\n",
            "person_8_has_property: https://ipfs.io/ipfs/bafkreihqie7whgxt3gabe4r6vercfppdrahan5mf3xkr6ut3uww3gus5ny\n",
            "property_has_layout_0: https://ipfs.io/ipfs/bafkreicsa6rlrcbtmuvdi6ukp3qrdkbvhabqz5zohazanr5re6ccmgkkii\n",
            "property_has_layout_1: https://ipfs.io/ipfs/bafkreicsa6rlrcbtmuvdi6ukp3qrdkbvhabqz5zohazanr5re6ccmgkkii\n",
            "property_has_layout_2: https://ipfs.io/ipfs/bafkreihsqiejmtd3upibxewlh42iebw5p7i57umqzjiahvory2eewvicji\n",
            "property_has_layout_3: https://ipfs.io/ipfs/bafkreihsqiejmtd3upibxewlh42iebw5p7i57umqzjiahvory2eewvicji\n",
            "property_has_tax_0: https://ipfs.io/ipfs/bafkreia2te2cjodecrvcavv5jlbgbouxdvell26sa3xycxcqibce7sxa5q\n",
            "property_has_tax_1: https://ipfs.io/ipfs/bafkreibfpcgqt7kghbccqofkuulvclrgestltbq7tfm6mwvmdppahrj7iu\n",
            "property_has_tax_2: https://ipfs.io/ipfs/bafkreiblepqt256dbuecsxy4374ddjz66lfsedmqfs2c3fqazbpo2ynyd4\n",
            "property_has_tax_3: https://ipfs.io/ipfs/bafkreicdphzbof7fyaac6jynobd3pzrvloq2t2zgtyl44pkxz26watcvwa\n",
            "property_has_tax_4: https://ipfs.io/ipfs/bafkreicor7togiqvdpnikkzyluvp56pqgvllbu7zn2qzxuwqdyhd7y272a\n",
            "property_has_tax_5: https://ipfs.io/ipfs/bafkreig2kiojdtt7onhd5oy7m7x7rzasbfn4jfk7n3eileaa72ukdtukb4\n",
            "property_has_tax_6: https://ipfs.io/ipfs/bafkreig5ow3dc3mimibqvc5n6vqtcy2wiuychpd3vtqr4grqtsw7sz3dba\n",
            "property_has_tax_7: https://ipfs.io/ipfs/bafkreigs2qk47qvcfnu54g74xfqldugzkvrbl2b5autiqsikdkfsrqy2he\n",
            "property_has_tax_8: https://ipfs.io/ipfs/bafkreihpaxfflb4xq7h7uvfdcfjd2lbdyro55q6yalccwgmh4wuhuv7maq\n",
            "property_has_tax_9: https://ipfs.io/ipfs/bafkreihpw34scqefjpjjkicbxdk4agzsenukwk7wxn4fm5h5ryhlpvrmtm\n",
            "property_has_sales_history_0: https://ipfs.io/ipfs/bafkreib4ukztgvyylrxns7sjk4emastskyrqzvilqlx2amzthrymwxfdje\n",
            "property_has_sales_history_1: https://ipfs.io/ipfs/bafkreibeh4bi432bkbcnycjmayiokfs722hxfpmn3gcazsanzvawphef3q\n",
            "property_has_sales_history_2: https://ipfs.io/ipfs/bafkreibglvgcdwfugb2bucsazizgen6xuavkn5jp4remubqjvogvt6mtf4\n",
            "property_has_sales_history_3: https://ipfs.io/ipfs/bafkreibxnbeuabwmiubngqsra5bqe2ydkz2jbvmxwra3wsbnayndko5nju\n",
            "property_has_sales_history_4: https://ipfs.io/ipfs/bafkreichuow3zsk7zxkst6rmhq2ligkug3er3b5vq2ynkzxm4buwppl5ky\n",
            "property_has_sales_history_5: https://ipfs.io/ipfs/bafkreie7pxea2nrx5lgcclbibb6dup67lg6mjseen4uyiywul2cbu5zhcq\n",
            "property_has_sales_history_6: https://ipfs.io/ipfs/bafkreiedcoyw5twlp2hsnonagbb6lllnjt7lrendizhsswooibzxrf4gk4\n",
            "property_has_sales_history_7: https://ipfs.io/ipfs/bafkreifweluqb4pjzifkui4yl6xvjg26hp5v6zcn6xsjtosmkvhkodsjfa\n",
            "property_has_sales_history_8: https://ipfs.io/ipfs/bafkreifwv267ehngnyqddxzgnghayyucmi44dr4srtbv2ugunn5bgbubau\n",
            "property_has_sales_history_9: https://ipfs.io/ipfs/bafkreigcsja5toqutkruqvijpn5m6fbfm4s3xlwfuyecq5bn2qlv2rebky\n",
            "property_has_sales_history_10: https://ipfs.io/ipfs/bafkreigouc4unnfxilpvplaioqyiysz7gnnihjxagpxbk2efzmh5j7jr5y\n",
            "property_has_sales_history_11: https://ipfs.io/ipfs/bafkreih2cmssw23bmayecliaffvm3uyvp3pi46tnadym2mwwzy3moqtzg4\n",
            "sales_history_has_company_0: https://ipfs.io/ipfs/bafkreiatkkpn34upf4olxlmlpwkpbaoos5brerpq24h33qj4bmtsezzvpe\n",
            "sales_history_has_person_0: https://ipfs.io/ipfs/bafkreiftvtjoaqndjz26x6d4mn7x43s3q76we7qowvx6nk37qcq7kelriq\n",
            "sales_history_has_person_1: https://ipfs.io/ipfs/bafkreigff54ycsngo2a47uegna7w6nazxicybwidytf2rbkwnkuq2cjqy4\n",
            "sales_history_has_person_2: https://ipfs.io/ipfs/bafkreigk2d2koijolhedb7g6fxfvqheu62yzekxnoges3wmb25qzgh34s4\n",
            "sales_history_has_person_3: https://ipfs.io/ipfs/bafkreigrgvq4lzdjhqplvtjt6vaia26ex4g7vdbn3hc5hrwboicmqacggq\n",
            "sales_history_has_person_4: https://ipfs.io/ipfs/bafkreih2optv57g52qdltefgqdjyfhbazccmivchwqfovrswjjnyeg35eu\n",
            "sales_history_has_person_5: https://ipfs.io/ipfs/bafkreih2vhtm7w62vflxqfreeksbg7syn5g25bxrfagohe7j6qgsfkneqq\n",
            "sales_history_has_person_6: https://ipfs.io/ipfs/bafkreihjmp4qrh2g3glewfwq7pq3yfd7jaubi5ev474yt4mnljsdmnzra4\n",
            "sales_history_has_person_7: https://ipfs.io/ipfs/bafkreihxadslrnnufmqc5lcq5bfpc3in34lg7qfkdcsi2fsrl4hdqzvzqu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7: Submit\n",
        "\n",
        "! pip3 install python-dotenv -q\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "\n",
        "def get_transaction_hash(path=\"transaction-status.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row[\"transactionHash\"]\n",
        "\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return next(reader, None) is not None\n",
        "\n",
        "\n",
        "def run_submit_to_contract():\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\n",
        "                \"npx\", \"-y\", \"@elephant-xyz/cli\", \"submit-to-contract\", \"county-results.csv\",\n",
        "                \"--from-address\", \"0xefAd08946612A15d5De8D4Db7fc03556b6424075\",\n",
        "                \"--api-key\", \"f7e18cf6-5d07-4e4a-ae23-f27b812614e6\",\n",
        "                \"--domain\", \"oracles-69c46050.staircaseapi.com\",\n",
        "                \"--oracle-key-id\", \"7ad26e0b-67c9-4c2f-95a2-2792c7db5ac7\",\n",
        "            ],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.PIPE,\n",
        "            check=True,\n",
        "            text=True,\n",
        "        )\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Submit failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        transaction_hash = get_transaction_hash()\n",
        "        transaction_link = f\"https://polygonscan.com/tx/{transaction_hash}\"\n",
        "\n",
        "        print(\"✅ Submit done\\n\")\n",
        "        print(f\"Transaction link: {transaction_link}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        print(e.stderr.strip(), file=sys.stderr)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_submit_to_contract()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "t3sh2FAwrs-U",
        "outputId": "efb0e75d-b16a-41a8-8955-9d71a3b9fad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Submit done\n",
            "\n",
            "Transaction link: https://polygonscan.com/tx/0x1a56c6a3f30c931e489d94902026e71d1981a438f91851c91061850982500d45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8: Download county-results.csv\n",
        "import os; from google.colab import files; (files.download('county-results.csv'), print(\"✅ File was downloaded successfully\"))[1] if os.path.exists('county-results.csv') else print(\"❌ File not found\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fNupfjnBr1U2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}