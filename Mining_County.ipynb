{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elephant-xyz/notebook/blob/main/Mining_County.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#County Mining process"
      ],
      "metadata": {
        "id": "jUcGqCQnZW9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Upload .env"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PI_XjroKd45G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Upload seed-results.csv"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1Btx2OEDfVtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3: Prepare\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from urllib.parse import urlencode\n",
        "from typing import Optional, Dict, Any\n",
        "import traceback\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PropertyDataProcessor:\n",
        "    def __init__(self, input_csv_path: str = \"seed-results.csv\", seed_csv_path: str = \"seed.csv\"):\n",
        "        self.input_csv_path = input_csv_path\n",
        "        self.seed_csv_path = seed_csv_path\n",
        "        self.processed_parcels = []  # Track all processed parcel IDs\n",
        "        self.ipfs_gateways = [\n",
        "            \"https://ipfs.io/ipfs/\",\n",
        "            \"https://gateway.pinata.cloud/ipfs/\",\n",
        "            \"https://cloudflare-ipfs.com/ipfs/\",\n",
        "            \"https://dweb.link/ipfs/\",\n",
        "            \"https://ipfs.infura.io/ipfs/\"\n",
        "        ]\n",
        "\n",
        "    def fetch_from_ipfs(self, cid: str) -> Optional[Dict[Any, Any]]:\n",
        "        \"\"\"Fetch data from IPFS using the provided CID with multiple gateway fallback.\"\"\"\n",
        "        for gateway in self.ipfs_gateways:\n",
        "            try:\n",
        "                url = f\"{gateway}{cid}\"\n",
        "                logger.info(f\"Trying to fetch {cid} from {gateway}\")\n",
        "                response = requests.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                return response.json()\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error fetching from {gateway}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.error(f\"Failed to fetch data from IPFS CID {cid} from all gateways\")\n",
        "        return None\n",
        "\n",
        "    def trace_ipfs_chain(self, data_cid: str) -> Optional[Dict[Any, Any]]:\n",
        "        \"\"\"Trace through the IPFS chain to get the final property data.\"\"\"\n",
        "\n",
        "        # Step 1: Fetch the initial data using dataCid\n",
        "        logger.info(f\"Step 1: Fetching initial data from dataCid: {data_cid}\")\n",
        "        initial_data = self.fetch_from_ipfs(data_cid)\n",
        "        if not initial_data:\n",
        "            return None\n",
        "\n",
        "        # Step 2: Extract property_seed CID from relationships\n",
        "        try:\n",
        "            property_seed_cid = initial_data[\"relationships\"][\"property_seed\"][\"/\"]\n",
        "            logger.info(f\"Step 2: Found property_seed CID: {property_seed_cid}\")\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Could not find property_seed CID in initial data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 3: Fetch property_seed data\n",
        "        logger.info(f\"Step 3: Fetching property_seed data from: {property_seed_cid}\")\n",
        "        property_seed_data = self.fetch_from_ipfs(property_seed_cid)\n",
        "        if not property_seed_data:\n",
        "            return None\n",
        "\n",
        "        # Step 4: Extract \"to\" CID from property_seed data\n",
        "        try:\n",
        "            to_cid = property_seed_data[\"to\"][\"/\"]\n",
        "            logger.info(f\"Step 4: Found 'to' CID: {to_cid}\")\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Could not find 'to' CID in property_seed data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 5: Fetch final property data\n",
        "        logger.info(f\"Step 5: Fetching final property data from: {to_cid}\")\n",
        "        final_data = self.fetch_from_ipfs(to_cid)\n",
        "\n",
        "        return final_data\n",
        "\n",
        "    def create_seed_csv(self):\n",
        "        \"\"\"Read the input CSV, trace IPFS chain, and create seed.csv.\"\"\"\n",
        "\n",
        "        # Read the input CSV\n",
        "        try:\n",
        "            df = pd.read_csv(self.input_csv_path)\n",
        "            logger.info(f\"Loaded {len(df)} records from {self.input_csv_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading CSV file: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Prepare output data\n",
        "        output_rows = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            data_cid = row['dataCid']\n",
        "            logger.info(f\"Processing row {index + 1}: {data_cid}\")\n",
        "\n",
        "            # Trace the IPFS chain\n",
        "            final_data = self.trace_ipfs_chain(data_cid)\n",
        "\n",
        "            if final_data:\n",
        "                try:\n",
        "                    # Extract data for CSV\n",
        "                    parcel_id = final_data.get('request_identifier', '')\n",
        "                    address = final_data.get('full_address', '')\n",
        "                    county = final_data.get('county_jurisdiction', '')\n",
        "\n",
        "                    # Track this parcel ID\n",
        "                    if parcel_id:\n",
        "                        self.processed_parcels.append(parcel_id)\n",
        "\n",
        "                    # Extract HTTP request details\n",
        "                    http_request = final_data.get('source_http_request', {})\n",
        "                    method = http_request.get('method', '')\n",
        "                    url = http_request.get('url', '')\n",
        "                    multi_value_query_string = http_request.get('multiValueQueryString', {})\n",
        "\n",
        "                    # Convert multiValueQueryString to JSON string for CSV\n",
        "                    multi_value_query_string_str = json.dumps(multi_value_query_string) if multi_value_query_string else ''\n",
        "\n",
        "                    # Create output row\n",
        "                    output_row = {\n",
        "                        'parcel_id': parcel_id,\n",
        "                        'address': address,\n",
        "                        'method': method,\n",
        "                        'headers': '',  # Empty as per example\n",
        "                        'url': url,\n",
        "                        'multiValueQueryString': multi_value_query_string_str,\n",
        "                        'body': '',  # Empty as per example\n",
        "                        'json': '',  # Empty as per example\n",
        "                        'source_identifier': parcel_id,  # Same as parcel_id based on example\n",
        "                        'County': county\n",
        "                    }\n",
        "\n",
        "                    output_rows.append(output_row)\n",
        "                    logger.info(f\"Successfully processed parcel ID: {parcel_id}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing final data for row {index + 1}: {e}\")\n",
        "            else:\n",
        "                logger.error(f\"Failed to trace IPFS chain for row {index + 1}\")\n",
        "\n",
        "        # Create output DataFrame and save to CSV\n",
        "        if output_rows:\n",
        "            output_df = pd.DataFrame(output_rows)\n",
        "            output_df.to_csv(self.seed_csv_path, index=False)\n",
        "            return True\n",
        "        else:\n",
        "            logger.error(\"No data was successfully processed\")\n",
        "            print(\"No data was successfully processed\")\n",
        "            return False\n",
        "\n",
        "    def create_output_directory(self):\n",
        "        \"\"\"Create the input directory if it doesn't exist\"\"\"\n",
        "        if not os.path.exists('input'):\n",
        "            os.makedirs('input')\n",
        "            logger.info(\"Created 'input' directory\")\n",
        "\n",
        "    def parse_multi_value_query_string(self, query_string_json):\n",
        "        \"\"\"Parse the multiValueQueryString JSON and convert to URL parameters\"\"\"\n",
        "        try:\n",
        "            if not query_string_json or query_string_json.strip() == '':\n",
        "                return {}\n",
        "\n",
        "            query_data = json.loads(query_string_json)\n",
        "            # Convert multi-value query string to regular query parameters\n",
        "            params = {}\n",
        "            for key, values in query_data.items():\n",
        "                if isinstance(values, list) and len(values) > 0:\n",
        "                    params[key] = values[0]  # Take the first value\n",
        "                else:\n",
        "                    params[key] = values\n",
        "            return params\n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.error(f\"Error parsing query string JSON: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def make_request(self, row):\n",
        "        \"\"\"Make HTTP request based on CSV row data\"\"\"\n",
        "        try:\n",
        "            parcel_id = row['parcel_id']\n",
        "            address = row['address']\n",
        "            method = row['method'].upper()\n",
        "            url = row['url']\n",
        "            query_params = self.parse_multi_value_query_string(row['multiValueQueryString'])\n",
        "\n",
        "            logger.info(f\"Processing parcel {parcel_id} at {address}\")\n",
        "\n",
        "            # Use headers from CSV if provided, otherwise use minimal headers\n",
        "            request_headers = {}\n",
        "            if row.get('headers') and row['headers'].strip():\n",
        "                try:\n",
        "                    request_headers = json.loads(row['headers'])\n",
        "                except json.JSONDecodeError:\n",
        "                    logger.warning(f\"Invalid headers JSON for parcel {parcel_id}, using minimal headers\")\n",
        "\n",
        "            # If no headers provided or parsing failed, use minimal headers\n",
        "            if not request_headers:\n",
        "                request_headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                }\n",
        "\n",
        "            # Make the request\n",
        "            if method == 'GET':\n",
        "                response = requests.get(url, params=query_params, headers=request_headers, timeout=30)\n",
        "            elif method == 'POST':\n",
        "                # Handle POST request with body if provided\n",
        "                post_data = {}\n",
        "                if row.get('body') and row['body'].strip():\n",
        "                    try:\n",
        "                        post_data = json.loads(row['body'])\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"Invalid body JSON for parcel {parcel_id}, using empty body\")\n",
        "\n",
        "                # Handle JSON data if provided\n",
        "                if row.get('json') and row['json'].strip():\n",
        "                    try:\n",
        "                        json_data = json.loads(row['json'])\n",
        "                        response = requests.post(url, params=query_params, headers=request_headers, json=json_data, timeout=30)\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"Invalid JSON data for parcel {parcel_id}, using form data\")\n",
        "                        response = requests.post(url, params=query_params, headers=request_headers, data=post_data, timeout=30)\n",
        "                else:\n",
        "                    response = requests.post(url, params=query_params, headers=request_headers, data=post_data, timeout=30)\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported method {method} for parcel {parcel_id}\")\n",
        "                return False\n",
        "\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Save the HTML content\n",
        "            filename = f\"input/{parcel_id}.html\"\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(response.text)\n",
        "\n",
        "            logger.info(f\"Successfully saved {filename}\")\n",
        "            return True\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Request failed for parcel {parcel_id}: {e}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error processing parcel {parcel_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_property_data(self):\n",
        "        \"\"\"Read seed CSV and download property data for each parcel\"\"\"\n",
        "        successful_downloads = 0\n",
        "        failed_downloads = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.seed_csv_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "\n",
        "                # Print available columns for debugging\n",
        "                logger.info(f\"Available columns: {reader.fieldnames}\")\n",
        "\n",
        "                # Verify required columns exist\n",
        "                required_columns = ['parcel_id', 'address', 'method', 'url', 'multiValueQueryString']\n",
        "                missing_columns = [col for col in required_columns if col not in reader.fieldnames]\n",
        "                if missing_columns:\n",
        "                    logger.error(f\"Missing required columns: {missing_columns}\")\n",
        "                    return False\n",
        "\n",
        "                logger.info(f\"Starting to process seed CSV file: {self.seed_csv_path}\")\n",
        "\n",
        "                # Convert reader to list to see total count\n",
        "                rows = list(reader)\n",
        "                total_rows = len(rows)\n",
        "                logger.info(f\"Found {total_rows} rows to process\")\n",
        "\n",
        "                for row_num, row in enumerate(rows, start=1):\n",
        "                    logger.info(f\"Processing row {row_num}/{total_rows} - Parcel: {row.get('parcel_id', 'Unknown')}\")\n",
        "\n",
        "                    try:\n",
        "                        if self.make_request(row):\n",
        "                            successful_downloads += 1\n",
        "                            logger.info(f\"✓ Successfully processed parcel {row.get('parcel_id')}\")\n",
        "                        else:\n",
        "                            failed_downloads += 1\n",
        "                            logger.error(f\"✗ Failed to process parcel {row.get('parcel_id')}\")\n",
        "                    except Exception as e:\n",
        "                        failed_downloads += 1\n",
        "                        logger.error(f\"✗ Exception processing parcel {row.get('parcel_id')}: {e}\")\n",
        "\n",
        "                    # Add a small delay to be respectful to the server\n",
        "                    time.sleep(1)\n",
        "\n",
        "            logger.info(f\"Download complete. Successful: {successful_downloads}, Failed: {failed_downloads}\")\n",
        "            return True\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Seed CSV file '{self.seed_csv_path}' not found\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing seed CSV file: {e}\")\n",
        "            logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def run_complete_process(self):\n",
        "        \"\"\"Run the complete process: IPFS data fetching + property download\"\"\"\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"STARTING COMPLETE PROPERTY DATA PROCESSING\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        # Step 1: Create seed CSV from IPFS data\n",
        "        logger.info(\"STEP 1: Processing IPFS data to create seed CSV...\")\n",
        "        if not self.create_seed_csv():\n",
        "            logger.error(\"Failed to create seed CSV. Aborting.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"STEP 1 COMPLETED: Seed CSV created successfully\")\n",
        "        logger.info(\"-\" * 40)\n",
        "\n",
        "        # Step 2: Create output directory for HTML files\n",
        "        logger.info(\"STEP 2: Creating output directory...\")\n",
        "        self.create_output_directory()\n",
        "        logger.info(\"STEP 2 COMPLETED: Output directory ready\")\n",
        "        logger.info(\"-\" * 40)\n",
        "\n",
        "        # Step 3: Download property data\n",
        "        logger.info(\"STEP 3: Downloading property data from county websites...\")\n",
        "        if not self.download_property_data():\n",
        "            logger.error(\"Failed to download property data.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"STEP 3 COMPLETED: Property data download finished\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"COMPLETE PROCESS FINISHED SUCCESSFULLY\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the complete property data processor\"\"\"\n",
        "\n",
        "    # Initialize the processor with default file paths\n",
        "    # You can modify these paths as needed\n",
        "    processor = PropertyDataProcessor(\n",
        "        input_csv_path=\"seed-results.csv\",  # Input CSV with dataCid column\n",
        "        seed_csv_path=\"seed.csv\"              # Output seed CSV and input for downloads\n",
        "    )\n",
        "\n",
        "    # Run the complete process\n",
        "    success = processor.run_complete_process()\n",
        "\n",
        "    if success:\n",
        "        # Show all processed parcel IDs\n",
        "        if processor.processed_parcels:\n",
        "            parcels_str = \", \".join(processor.processed_parcels)\n",
        "            print(f\"✅ Prepare done for parcel IDs: {parcels_str}\")\n",
        "        else:\n",
        "            print(\"✅ Prepare done (no parcel IDs found)\")\n",
        "    else:\n",
        "        print(\"❌ Prepare Failed\")\n",
        "        print(\"Check the logs above for detailed error information\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p9NoMY6ufmSE",
        "collapsed": true,
        "outputId": "31481ae7-ac4d-4217-9a4c-58ca547a0f67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Prepare done for parcel IDs: 52434205310037080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Transform\n",
        "#!/usr/bin/env python3\n",
        "import subprocess\n",
        "import sys\n",
        "import shutil\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies\"\"\"\n",
        "    try:\n",
        "        subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \"python-dotenv\", \"-q\"\n",
        "        ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        return True\n",
        "    except:\n",
        "        print(\"❌ Failed to install dependencies\")\n",
        "        return False\n",
        "\n",
        "def load_environment():\n",
        "    \"\"\"Load environment variables from .env file\"\"\"\n",
        "    try:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"❌ Failed to import dotenv\")\n",
        "        return False\n",
        "\n",
        "def check_env_file():\n",
        "    \"\"\"Check if .env file exists\"\"\"\n",
        "    if not os.path.exists(\".env\"):\n",
        "        print(\"❌ Transform failed: .env file not found\")\n",
        "        print(\"Please upload the .env file to continue\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def copy_seed_results():\n",
        "    \"\"\"Copy seed-results.csv to upload-results.csv\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(\"seed-results.csv\"):\n",
        "            print(\"❌ Transform failed: seed-results.csv not found\")\n",
        "            return False\n",
        "\n",
        "        shutil.copy2(\"seed-results.csv\", \"upload-results.csv\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ Transform failed: Could not copy seed-results.csv\")\n",
        "        return False\n",
        "\n",
        "def run_transform():\n",
        "    \"\"\"Run the transform command and suppress logs\"\"\"\n",
        "\n",
        "    command = [\n",
        "        \"uvx\",\n",
        "        \"--from\",\n",
        "        \"git+https://github.com/elephant-xyz/AI-Agent\",\n",
        "        \"test-evaluator-agent\",\n",
        "        \"--transform\"\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(\"🔄 Transforming running...\")\n",
        "\n",
        "        # Run the command and suppress all output\n",
        "        result = subprocess.run(\n",
        "            command,\n",
        "            stdout=subprocess.DEVNULL,  # Suppress stdout\n",
        "            stderr=subprocess.DEVNULL,  # Suppress stderr\n",
        "            check=True  # Raise exception if command fails\n",
        "        )\n",
        "\n",
        "        return True\n",
        "\n",
        "    except:\n",
        "        print(\"❌ Transform failed\")\n",
        "        return False\n",
        "\n",
        "def get_seed_cid_and_html_link(path=\"county-results.csv\"):\n",
        "    \"\"\"Get County CID and HTML link from CSV file\"\"\"\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row[\"dataGroupCid\"], first_row[\"htmlLink\"]\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    \"\"\"\n",
        "    Returns True if submit_errors.csv has at least one row (after header).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            return next(reader, None) is not None\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "\n",
        "def run_validate_and_upload():\n",
        "    \"\"\"Run validation and upload process\"\"\"\n",
        "    try:\n",
        "\n",
        "        subprocess.run(\n",
        "            [\"npx\", \"-y\", \"@elephant-xyz/cli\", \"validate-and-upload\", \"submit\", \"--output-csv\", \"county-results.csv\"],\n",
        "            stdout=subprocess.DEVNULL,    # hide stdout\n",
        "            stderr=subprocess.PIPE,       # capture stderr\n",
        "            check=True,\n",
        "            text=True                     # stderr as string\n",
        "        )\n",
        "\n",
        "        # If there are recorded errors - stop execution\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Transform failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return False\n",
        "\n",
        "        # Otherwise - read results\n",
        "        seed_group_cid, html_link = get_seed_cid_and_html_link()\n",
        "        print(\"✅ Transform done\\n\")\n",
        "        print(f\"county group CID: {seed_group_cid}\\n\")\n",
        "        print(f\"HTML link: {html_link}\")\n",
        "        return True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # handle command execution errors\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        if e.stderr:\n",
        "            print(e.stderr.strip(), file=sys.stderr)\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Validation and upload failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    # Step 1: Install dependencies\n",
        "    if not install_dependencies():\n",
        "        return False\n",
        "\n",
        "    # Step 2: Load environment\n",
        "    if not load_environment():\n",
        "        return False\n",
        "\n",
        "    # Step 3: Check for .env file\n",
        "    if not check_env_file():\n",
        "        return False\n",
        "\n",
        "    # Step 4: Copy seed-results.csv to upload-results.csv\n",
        "    if not copy_seed_results():\n",
        "        return False\n",
        "\n",
        "    # Step 5: Run the transform command\n",
        "    if not run_transform():\n",
        "        return False\n",
        "\n",
        "    # Step 6: Run validation and upload\n",
        "    success = run_validate_and_upload()\n",
        "    return success\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    # Don't use sys.exit() in Jupyter/IPython environments\n",
        "\n",
        "    if not success:\n",
        "        print(\"Process completed with errors\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DyG0lJvbmcej",
        "collapsed": true,
        "outputId": "58566f82-ee4d-4e65-d11b-760c581f477e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Transforming running...\n",
            "✅ Transform done\n",
            "\n",
            "county group CID: bafkreie5pbx4k3wt3fnd4qewthsde2jxewm3krcgn72ecbyvnzqhaeylce\n",
            "\n",
            "HTML link: http://dweb.link/ipfs/bafybeidda22io3lqdhhrvzenibd4za6xv7kl5scpdo3twnttwb6rtcxrrq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5: Validate\n",
        "! pip3 install python-dotenv -q\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "\n",
        "def get_seed_cid_and_html_link(path=\"county-results.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row[\"dataGroupCid\"], first_row[\"htmlLink\"]\n",
        "\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    \"\"\"\n",
        "    Повертає True, якщо у файлі submit_errors.csv є хоча б один рядок (після заголовку).\n",
        "    \"\"\"\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return next(reader, None) is not None\n",
        "\n",
        "\n",
        "def run_validate_and_upload():\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"npx\", \"-y\", \"@elephant-xyz/cli\", \"validate-and-upload\", \"submit\", \"--output-csv\", \"county-results.csv\"],\n",
        "            stdout=subprocess.DEVNULL,    # ховаємо stdout\n",
        "            stderr=subprocess.PIPE,       # ловимо stderr у буфер\n",
        "            check=True,\n",
        "            text=True                     # stderr як рядок\n",
        "        )\n",
        "        # Якщо є записані помилки — завершуємо роботу\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Validate failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        # Інакше — читаємо результати\n",
        "        seed_group_cid, html_link = get_seed_cid_and_html_link()\n",
        "        print(\"✅ Validate done\\n\")\n",
        "        print(f\"County group CID: {seed_group_cid}\\n\")\n",
        "        print(f\"HTML link: {html_link}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # обробка помилок виконання команди\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        print(e.stderr.strip(), file=sys.stderr)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_validate_and_upload()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "82AWqrDApExu",
        "outputId": "f40ae643-420a-48c9-d914-835da4e94fb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validate done\n",
            "\n",
            "County group CID: bafkreie5pbx4k3wt3fnd4qewthsde2jxewm3krcgn72ecbyvnzqhaeylce\n",
            "\n",
            "HTML link: http://dweb.link/ipfs/bafybeifpijxir6ymclazg52vyehudwb6x34zdiyehkx23bhfxrsteddpa4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6: Upload\n",
        "! pip3 install python-dotenv requests -q\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def get_seed_info(path=\"county-results.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row\n",
        "\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    \"\"\"\n",
        "    Повертає True, якщо у файлі submit_errors.csv є хоча б один рядок (після заголовку).\n",
        "    \"\"\"\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return next(reader, None) is not None\n",
        "\n",
        "\n",
        "def count_upload_records(path=\"county-results.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return sum(1 for _ in reader)\n",
        "\n",
        "\n",
        "def fetch_with_fallback(cid, gateways=None):\n",
        "    \"\"\"\n",
        "    Try to fetch IPFS content from multiple gateways with fallback\n",
        "    \"\"\"\n",
        "    if gateways is None:\n",
        "        gateways = [\n",
        "            \"https://ipfs.io/ipfs/\",\n",
        "            \"https://gateway.pinata.cloud/ipfs/\",\n",
        "            \"https://dweb.link/ipfs/\",\n",
        "            \"https://cloudflare-ipfs.com/ipfs/\"\n",
        "        ]\n",
        "\n",
        "    for gateway in gateways:\n",
        "        try:\n",
        "            url = f\"{gateway}{cid}\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200 and response.text.strip():\n",
        "                return response\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    # If all gateways fail, return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def collect_data_ipfs_links(data_cid):\n",
        "    \"\"\"\n",
        "    Collect IPFS links for County data structure\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = fetch_with_fallback(data_cid)\n",
        "        if response is None:\n",
        "            print(f\"Error: Could not fetch seed data from any gateway for CID: {data_cid}\", file=sys.stderr)\n",
        "            return {}\n",
        "        seed_data = response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching seed data: {e}\", file=sys.stderr)\n",
        "        return {}\n",
        "\n",
        "    entity_links = {}  # For actual entities (person, property, etc.)\n",
        "    relationship_links = {}  # For relationship objects\n",
        "    url_to_name = {}  # Track which URLs we've already seen and their preferred names\n",
        "\n",
        "    # Get relationships from the County data\n",
        "    relationships = seed_data.get(\"relationships\", {})\n",
        "\n",
        "    # Process required single-value relationships\n",
        "    for rel_name in [\"property_has_address\", \"property_has_lot\", \"property_has_structure\", \"property_has_utility\", \"property_has_flood_storm_information\"]:\n",
        "        if rel_name in relationships and relationships[rel_name]:\n",
        "            rel_cid = relationships[rel_name].get(\"/\") if isinstance(relationships[rel_name], dict) else relationships[rel_name]\n",
        "            if rel_cid:\n",
        "                # Try to get the referenced data and extract 'to' and 'from' fields FIRST\n",
        "                try:\n",
        "                    response = fetch_with_fallback(rel_cid)\n",
        "                    if response is None:\n",
        "                        print(f\"Warning: Could not fetch {rel_name} from any gateway: {rel_cid}\", file=sys.stderr)\n",
        "                        continue\n",
        "\n",
        "                    rel_data = response.json()\n",
        "\n",
        "                    if \"to\" in rel_data and \"/\" in rel_data[\"to\"]:\n",
        "                        to_cid = rel_data[\"to\"][\"/\"]\n",
        "                        # Extract x and y from x_has_y pattern\n",
        "                        parts = rel_name.split(\"_has_\")\n",
        "                        x = parts[0] if len(parts) > 0 else \"unknown\"\n",
        "                        y = parts[1] if len(parts) > 1 else \"unknown\"\n",
        "\n",
        "                        # Handle 'from' URL\n",
        "                        if \"from\" in rel_data and \"/\" in rel_data[\"from\"]:\n",
        "                            from_cid = rel_data['from']['/']\n",
        "                            from_url = f\"https://ipfs.io/ipfs/{from_cid}\"\n",
        "                            if from_url not in url_to_name:\n",
        "                                url_to_name[from_url] = x\n",
        "                                entity_links[x] = from_url\n",
        "\n",
        "                        # Handle 'to' URL\n",
        "                        to_url = f\"https://ipfs.io/ipfs/{to_cid}\"\n",
        "                        if to_url not in url_to_name:\n",
        "                            url_to_name[to_url] = y\n",
        "                            entity_links[y] = to_url\n",
        "\n",
        "                    # Add the relationship link itself AFTER processing entities\n",
        "                    relationship_links[rel_name] = f\"https://ipfs.io/ipfs/{rel_cid}\"\n",
        "\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: JSON decode error for {rel_name}: {e}\", file=sys.stderr)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not fetch relationship data for {rel_name}: {e}\", file=sys.stderr)\n",
        "\n",
        "    # Process array relationships\n",
        "    array_rels = [\n",
        "        \"company_has_property\",\n",
        "        \"person_has_property\",\n",
        "        \"property_has_file\",\n",
        "        \"property_has_layout\",\n",
        "        \"property_has_tax\",\n",
        "        \"property_has_sales_history\",\n",
        "        \"sales_history_has_company\",\n",
        "        \"sales_history_has_person\"\n",
        "    ]\n",
        "\n",
        "    for rel_name in array_rels:\n",
        "        if rel_name in relationships and relationships[rel_name]:\n",
        "            rel_array = relationships[rel_name]\n",
        "            if isinstance(rel_array, list):\n",
        "                for i, rel_item in enumerate(rel_array):\n",
        "                    rel_cid = rel_item.get(\"/\") if isinstance(rel_item, dict) else rel_item\n",
        "                    if rel_cid:\n",
        "                        # Try to get the referenced data and extract 'to' and 'from' fields FIRST\n",
        "                        try:\n",
        "                            response = fetch_with_fallback(rel_cid)\n",
        "                            if response is None:\n",
        "                                print(f\"Warning: Could not fetch {rel_name}[{i}] from any gateway: {rel_cid}\", file=sys.stderr)\n",
        "                                continue\n",
        "\n",
        "                            rel_data = response.json()\n",
        "\n",
        "                            if \"to\" in rel_data and \"/\" in rel_data[\"to\"]:\n",
        "                                to_cid = rel_data[\"to\"][\"/\"]\n",
        "                                # Extract x and y from x_has_y pattern\n",
        "                                parts = rel_name.split(\"_has_\")\n",
        "                                x = parts[0] if len(parts) > 0 else \"unknown\"\n",
        "                                y = parts[1] if len(parts) > 1 else \"unknown\"\n",
        "\n",
        "                                # Handle 'from' URL\n",
        "                                if \"from\" in rel_data and \"/\" in rel_data[\"from\"]:\n",
        "                                    from_cid = rel_data['from']['/']\n",
        "                                    from_url = f\"https://ipfs.io/ipfs/{from_cid}\"\n",
        "                                    if from_url not in url_to_name:\n",
        "                                        # Use index for arrays to make keys unique only if needed\n",
        "                                        x_key = f\"{x}_{i+1}\" if x in entity_links else x\n",
        "                                        url_to_name[from_url] = x_key\n",
        "                                        entity_links[x_key] = from_url\n",
        "\n",
        "                                # Handle 'to' URL\n",
        "                                to_url = f\"https://ipfs.io/ipfs/{to_cid}\"\n",
        "                                if to_url not in url_to_name:\n",
        "                                    # Use index for arrays to make keys unique only if needed\n",
        "                                    y_key = f\"{y}_{i+1}\" if y in entity_links else y\n",
        "                                    url_to_name[to_url] = y_key\n",
        "                                    entity_links[y_key] = to_url\n",
        "\n",
        "                            # Add the relationship link itself with index AFTER processing entities\n",
        "                            rel_key = f\"{rel_name}_{i}\"\n",
        "\n",
        "                            # Special naming for person_has_property and company_has_property relationships\n",
        "                            if rel_name == \"person_has_property\":\n",
        "                                rel_key = f\"person_{i+1}_has_property\"\n",
        "                            elif rel_name == \"company_has_property\":\n",
        "                                rel_key = f\"company_{i+1}_has_property\"\n",
        "\n",
        "                            relationship_links[rel_key] = f\"https://ipfs.io/ipfs/{rel_cid}\"\n",
        "\n",
        "                        except ValueError as e:\n",
        "                            print(f\"Warning: JSON decode error for {rel_name}[{i}]: {e}\", file=sys.stderr)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Could not fetch relationship data for {rel_name}[{i}]: {e}\", file=sys.stderr)\n",
        "\n",
        "    # Combine entity links first, then relationship links\n",
        "    all_links = {}\n",
        "    all_links.update(entity_links)\n",
        "    all_links.update(relationship_links)\n",
        "\n",
        "    return all_links\n",
        "\n",
        "\n",
        "def run_validate_and_upload():\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\"npx\", \"-y\", \"@elephant-xyz/cli\", \"validate-and-upload\", \"submit\", \"--output-csv\", \"county-results.csv\"],\n",
        "            stdout=subprocess.DEVNULL,    # ховаємо stdout\n",
        "            stderr=subprocess.PIPE,       # ловимо stderr у буфер\n",
        "            check=True,\n",
        "            text=True,\n",
        "        )\n",
        "\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Validate failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        seed_info = get_seed_info()\n",
        "        seed_group_cid, data_cid, html_link = seed_info[\"dataGroupCid\"], seed_info[\"dataCid\"], seed_info[\"htmlLink\"]\n",
        "\n",
        "        all_links = collect_data_ipfs_links(data_cid)\n",
        "\n",
        "        files_uploaded = len(all_links)\n",
        "\n",
        "        print(\"✅ Upload done\\n\")\n",
        "        print(f\"{files_uploaded} files uploaded\\n\")\n",
        "\n",
        "        print(f\"County group CID: {seed_group_cid}\\n\")\n",
        "        print(f\"HTML link: {html_link}\\n\")\n",
        "\n",
        "        # Print all collected IPFS links (now deduplicated)\n",
        "        print(\"=== IPFS Links ===\")\n",
        "        for link_name, link_url in all_links.items():\n",
        "            print(f\"{link_name}: {link_url}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        print(e.stderr.strip(), file=sys.stderr)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_validate_and_upload()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "IgDkOErzrfj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7: Submit\n",
        "\n",
        "! pip3 install python-dotenv -q\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "\n",
        "def get_transaction_hash(path=\"transaction-status.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        first_row = next(reader, None)\n",
        "        if first_row is None:\n",
        "            raise ValueError(\"CSV file is empty\")\n",
        "        return first_row[\"transactionHash\"]\n",
        "\n",
        "\n",
        "def has_submit_errors(path=\"submit_errors.csv\"):\n",
        "    with open(path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return next(reader, None) is not None\n",
        "\n",
        "\n",
        "def run_submit_to_contract():\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            [\n",
        "                \"npx\", \"-y\", \"@elephant-xyz/cli\", \"submit-to-contract\", \"county-results.csv\",\n",
        "                \"--from-address\", \"0xefAd08946612A15d5De8D4Db7fc03556b6424075\",\n",
        "                \"--api-key\", \"f7e18cf6-5d07-4e4a-ae23-f27b812614e6\",\n",
        "                \"--domain\", \"oracles-69c46050.staircaseapi.com\",\n",
        "                \"--oracle-key-id\", \"7ad26e0b-67c9-4c2f-95a2-2792c7db5ac7\",\n",
        "            ],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.PIPE,\n",
        "            check=True,\n",
        "            text=True,\n",
        "        )\n",
        "        if has_submit_errors():\n",
        "            print(\"❌ Submit failed, please check submit_errors.csv for details\", file=sys.stderr)\n",
        "            return\n",
        "\n",
        "        transaction_hash = get_transaction_hash()\n",
        "        transaction_link = f\"https://polygonscan.com/tx/{transaction_hash}\"\n",
        "\n",
        "        print(\"✅ Submit done\\n\")\n",
        "        print(f\"Transaction link: {transaction_link}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed (exit code {e.returncode}):\", file=sys.stderr)\n",
        "        print(e.stderr.strip(), file=sys.stderr)\n",
        "        sys.exit(e.returncode)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_submit_to_contract()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t3sh2FAwrs-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8: Download county-results.csv\n",
        "import os; from google.colab import files; (files.download('county-results.csv'), print(\"✅ File was downloaded successfully\"))[1] if os.path.exists('county-results.csv') else print(\"❌ File not found\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fNupfjnBr1U2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Mining_County.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}